# Introduction Notes

## [Probability and Regression Review](https://mixtape.scunning.com/probability-and-regression.html#probability-and-regression)
* A random process is a process that can be repeated many times with different outcomes each time.
* "but there is no reason to believe that the two events affect each other. When it is assumed that they do affect each other, this is a logical fallacy called post hoc ergo propter hoc, which is Latin for “after this, therefore because of this.” This fallacy recognizes that the temporal ordering of events is not sufficient to be able to say that the first thing caused the second."
* Marginal probabilities introduced
  * This is a very important concept. What is the probability once you remove some other covariate
  * https://bayesiancomputationbook.com/_images/Neals_Funnel_Salad_Centered.png 

## [Events and Conditional Probability](https://mixtape.scunning.com/probability-and-regression.html#events-and-conditional-probability)
* $p^{c}$ is introduced
  * The notation in this book is going to be different

## [Probability Tree](https://mixtape.scunning.com/probability-and-regression.html#probability-tree)
* $p^{c}$ is introduced
  * The notation in this book is going to be different

## [Venn Diagrams and Sets](https://mixtape.scunning.com/probability-and-regression.html#venn-diagrams-and-sets)

## [Contingency Table](https://mixtape.scunning.com/probability-and-regression.html#contingency-tables)
* First intro of Bayes Theorem
  * Uses point estimate example

## [Monty Hall Example](https://mixtape.scunning.com/probability-and-regression.html#monty-hall-example)
* The notation in this chapter is rough because A and B are replaced as non informative var names
  * I much prefer $P(D2 is opened| D_{i} has a million)$
* https://www.nytimes.com/2008/04/08/science/08monty.html
* The thing to focus on here is the change in probability D2 is opened if D1, your original selection, has a million dollars behind it
* Really good example of how probabillity intuition can be very wrong
  * Even I struggled with this
* I dislilke as an Bayes Theorem now
* Stresses probability rules over inference
  * Can make people feel that Bayes Rule is applicable only in these brain teaser situations
  * Doesn't stress data generating process thinking, or generative process thinking
* "Whereas most of this book has to do with estimating effects from known causes, Bayes’s rule reminds us that we can form reasonable beliefs about causes from known effects."
  * If we see the data we can inform our priors

## [Summation Operator](https://mixtape.scunning.com/probability-and-regression.html#summation-operator)
* Average notation
* https://quicklatex.com/

## [Expected Value](https://mixtape.scunning.com/probability-and-regression.html#expected-value)
* Integral notation https://en.wikipedia.org/wiki/Expected_value#Random_variables_with_density

## [Variance](https://mixtape.scunning.com/probability-and-regression.html#variance)
* Degree of freedom operator is not used

## [Covariance](https://mixtape.scunning.com/probability-and-regression.html#covariance)
* When two events are not independent here is one way to measure the degree of their linear relationship

## [Population Model](https://mixtape.scunning.com/probability-and-regression.html#population-model)
* Linear model is introduced here 
* $u$ is mentioned as population model
* First assumption is stated, that $E(u)=0$

## [Mean Independence](https://mixtape.scunning.com/probability-and-regression.html#mean-independence)
* This section is confusing I need to come back to it. I don't really get what's being said here

